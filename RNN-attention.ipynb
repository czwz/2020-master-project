{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have these libraries\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def seq2vec(string, outlen, vocab):\n",
    "    \"\"\"\n",
    "    given dictionary, convet string to a vector.\n",
    "    the length of string should be shorter then outlen.\n",
    "    if the length string is smaller than outlen, it will be padded with zero to make the langth the same.\n",
    "    \n",
    "    string: input string (length smaller than outlen)\n",
    "    outlen: output length of vecotr.\n",
    "    vocab: the dictionary for the encoding.\n",
    "    \"\"\"   \n",
    "    vector = [vocab[amino_acid] for amino_acid in string]\n",
    "    vector = np.pad(vector, (0,outlen-len(vector)), constant_values=20)\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. load the data and tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"seq_data.csv\")\n",
    "vfile = pd.read_csv(\"vocab.csv\", index_col=None, skiprows=0)\n",
    "vocab = {vfile[\"One-Letter Code\"][i]:i for i in range(len(vfile[\"One-Letter Code\"]))}\n",
    "\n",
    "# Change vector to one-hot encoding\n",
    "vector = np.array([to_categorical(seq2vec(data[\"seq\"][i], 84, vocab)) for i in data.index])\n",
    "\n",
    "# Label the class according to scores\n",
    "data.loc[(data[\"binding score\"] + data[\"digest score\"] > 0), \"label\"] = 0\n",
    "data.loc[(data[\"binding score\"] + data[\"digest score\"] <= 0), \"label\"] = 1\n",
    "\n",
    "# Define constant and set up optimizer\n",
    "Tx = 84 # <- here i make all resulting vector with the same length, which is not necessarily.\n",
    "nbatch = 320\n",
    "nepoch = 1#00 # <- to just see whether it works or not, change it to 1\n",
    "adam = Adam(learning_rate=0.001)\n",
    "\n",
    "# Create training/testing set (80-20 split)\n",
    "sub_data = data.iloc[:]\n",
    "index = np.arange(len(sub_data))\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "train_index = index[:len(sub_data)*8//10]\n",
    "test_index = index[len(sub_data)*8//10:]\n",
    "\n",
    "trainX = vector[train_index]\n",
    "trainY = data[\"label\"][train_index].values\n",
    "testX = vector[test_index]\n",
    "testY = data[\"label\"][test_index].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. build the computation graph (LSTM with attention mechanism)\n",
    "##### This is the best model I have tested\n",
    "##### Let me know if you want something simpler (such as vanila RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computation graph below\n",
    "\n",
    "# Define inputs\n",
    "inputs = tf.keras.Input(shape=[Tx, len(vocab)], dtype='float32', name='Input')\n",
    "\n",
    "# Lstm layer\n",
    "activations = layers.Bidirectional(layers.LSTM(units=64,\n",
    "                                               kernel_regularizer=l2(1e-3),\n",
    "                                               recurrent_regularizer=l2(1e-4),\n",
    "                                               input_shape=(84, len(vocab)),\n",
    "                                               return_sequences=True,\n",
    "                                               name='LSTM_1'),\n",
    "                                   name='Bidirectional_1'\n",
    "                                  )(inputs)\n",
    "activations = layers.BatchNormalization(name='BN_1')(activations)\n",
    "activations = layers.Bidirectional(layers.LSTM(units=64,\n",
    "                                               kernel_regularizer=l2(1e-3),\n",
    "                                               recurrent_regularizer=l2(1e-4),\n",
    "                                               input_shape=(84, len(vocab)),\n",
    "                                               return_sequences=True,\n",
    "                                               name='LSTM_2'),\n",
    "                                   name='Bidirectional_2'\n",
    "                                  )(activations)\n",
    "activations = layers.BatchNormalization(name='BN_2')(activations)\n",
    "\n",
    "# Attention layer\n",
    "units = 128 # bidirectional\n",
    "attention = tf.keras.layers.Dense(1, activation='tanh', name='Attention_dense')(activations)\n",
    "attention = tf.keras.layers.Flatten(name='Attention_flatten')(attention)\n",
    "attention = tf.keras.layers.Activation('softmax', name='Attention_layer')(attention)\n",
    "attention = tf.keras.layers.RepeatVector(units, name='Attention_repeat')(attention)\n",
    "attention = tf.keras.layers.Permute([2, 1], name='Attention_permute')(attention)\n",
    "\n",
    "# Combine attention weights and lstm output\n",
    "attension_weights = tf.keras.layers.Multiply(name='Attention_apply')([activations, attention])\n",
    "representation = tf.keras.layers.Lambda(lambda xin: tf.keras.backend.sum(xin, axis=-2), name='Attention_sum')(attension_weights)\n",
    "\n",
    "# FC\n",
    "representation = layers.Dropout(0.1, input_shape=(64,), name='Dropout_1')(representation)\n",
    "representation = layers.Dense(units=64, activation='relu', name='FC_1')(representation)\n",
    "representation = layers.BatchNormalization(name='BN_3')(representation)\n",
    "representation = layers.Dropout(0.1, input_shape=(64,), name='Dropout_2')(representation)\n",
    "\n",
    "# Define output\n",
    "probabilities = tf.keras.layers.Dense(2, activation='softmax', name='FC_2')(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile and train the model\n",
    "##### Save it to \"saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = tf.keras.models.Model(inputs, probabilities)\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# See the summary\n",
    "model.summary()\n",
    "\n",
    "# train it\n",
    "model.fit(trainX, trainY, validation_data=(testX,testY), epochs=nepoch, batch_size=nbatch, verbose=1)\n",
    "model.save('saved_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the model just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application\n",
    "##### Such as make a prediction -- (1)\n",
    "##### Or see the attention weights -- (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1)\n",
    "predY = model.predict(testX)\n",
    "print(\"testing accuracy = {:5f}\".format(np.mean(np.argmax(predY, axis=1) == testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2)\n",
    "\n",
    "# load the attention weights\n",
    "keras_function = tf.keras.backend.function([model.input], [model.get_layer('Attention_layer').output])\n",
    "\n",
    "# see particular case such as sequecne no. 10141\n",
    "i = np.where(train_index == 10141)[0][0]\n",
    "weights = keras_function(trainX[i:i+1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
